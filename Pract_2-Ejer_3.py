# -*- coding: utf-8 -*-
"""Pract2-Eje2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SXVZXj83bjcRAEskYDvLyR3iGzYXRzv7
"""

#LIBRERIAS EN GENERAL
import pandas as pd
import statsmodels.api as sm
import numpy as np
import tensorflow as tf
import warnings
import matplotlib.pyplot as plt

from sklearn import svm
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score

from sklearn.preprocessing import StandardScaler,MinMaxScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
# Suprimir todos los warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('/content/winequality-white.csv')
df.hist()

"""# *REGRESION LOGISTICA*

---

"""

# DataFrame de auto
df = pd.read_csv('/content/AutoInsurSweden.csv')
X = df[['x','y']]
y = df[['z']]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
regression_model = LogisticRegression()
# Entrenar el modelo
regression_model.fit(X_train, y_train)
# Realizar predicciones
y_pred = regression_model.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()

#-------------------CALCULO DE METRICAS--------------------

conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)


# Calcular Accuracy
Accuracy = accuracy_score(y_test,y_pred)

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(Accuracy))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

#DataFrame de Diabetes
df = pd.read_csv('/content/Pima Indians Diabetes Dataset.csv')
df = df.dropna(subset=['Class'])
X = df.drop('Class',axis=1)
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
regression_model = LogisticRegression()
# Entrenar el modelo
regression_model.fit(X_train, y_train)
# Realizar predicciones
y_pred = regression_model.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

# DataFrame Vino
df = pd.read_csv('/content/winequality-white.csv')
df = df.dropna(subset=['quality'])
X = df.drop('quality',axis=1)
y = df['quality']
#df.hist()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
regression_model = LogisticRegression()
# Entrenar el modelo
regression_model.fit(X_train, y_train)
# Realizar predicciones
y_pred = regression_model.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión de manera más general
tn, fp, fn, tp = conf_matrix.ravel()[:4]

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

"""# *K-Vecinos Cercanos*

---

"""

# DataFrame de auto
df = pd.read_csv('/content/AutoInsurSweden.csv')
X = df[['x','y']]
y = df[['z']]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
n_neighbors = 7
knn = KNeighborsClassifier(n_neighbors)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

#DataFrame de Diabetes
df = pd.read_csv('/content/Pima Indians Diabetes Dataset.csv')
df = df.dropna(subset=['Class'])
X = df.drop('Class',axis=1)
y = df['Class']
#print(df.groupby('Class').size())
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
n_neighbors = 7
knn = KNeighborsClassifier(n_neighbors)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

# DataFrame Vino
df = pd.read_csv('/content/winequality-white.csv')
df = df.dropna(subset=['quality'])
X = df.drop('quality',axis=1)
y = df['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
n_neighbors = 7
knn = KNeighborsClassifier(n_neighbors)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión de manera más general
tn, fp, fn, tp = conf_matrix.ravel()[:4]

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

"""# *Máquinas de Vectores de Soporte*

---

"""

# DataFrame de auto
df = pd.read_csv('/content/AutoInsurSweden.csv')
X = df[['x','y']]
y = df[['z']]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
svm_classifier = svm.SVC(kernel='linear')
svm_classifier.fit(X_train, y_train)
y_pred = svm_classifier.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

#DataFrame de Diabetes
df = pd.read_csv('/content/Pima Indians Diabetes Dataset.csv')
df = df.dropna(subset=['Class'])
X = df.drop('Class',axis=1)
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
svm_classifier = svm.SVC(kernel='linear')
svm_classifier.fit(X_train, y_train)
y_pred = svm_classifier.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

# DataFrame Vino
df = pd.read_csv('/content/winequality-white.csv')
df = df.dropna(subset=['quality'])
X = df.drop('quality',axis=1)
y = df['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
svm_classifier = svm.SVC(kernel='linear')
svm_classifier.fit(X_train, y_train)
y_pred = svm_classifier.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión de manera más general
tn, fp, fn, tp = conf_matrix.ravel()[:4]

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

"""# *Naive Bayes*

---

"""

# DataFrame de auto
df = pd.read_csv('/content/AutoInsurSweden.csv')
X = df[['x','y']]
y = df[['z']]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
model = GaussianNB()
model.fit(X, y);
yprob = model.predict_proba(X_test)
y_pred = model.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()

#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

#DataFrame de Diabetes
df = pd.read_csv('/content/Pima Indians Diabetes Dataset.csv')
df = df.dropna(subset=['Class'])
X = df.drop('Class',axis=1)
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
model = GaussianNB()
model.fit(X, y);
yprob = model.predict_proba(X_test)
y_pred = model.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()

#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

# DataFrame Vino
df = pd.read_csv('/content/winequality-white.csv')
df = df.dropna(subset=['quality'])
X = df.drop('quality',axis=1)
y = df['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
model = GaussianNB()
model.fit(X, y);
yprob = model.predict_proba(X_test)
y_pred = model.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión de manera más general
tn, fp, fn, tp = conf_matrix.ravel()[:4]

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

"""# *Red Neuronal con Tensorflow*

---
"""

# DataFrame de auto
df = pd.read_csv('/content/AutoInsurSweden.csv')
X = df[['x','y']]
y = df[['z']]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Escalar los datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Puedes ajustar los hiperparámetros según tus necesidades
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)
mlp.fit(X_train, y_train)
# Hacer predicciones
y_pred = mlp.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

# DataFrame Diabetes
df = pd.read_csv('/content/Pima Indians Diabetes Dataset.csv')
df = df.dropna(subset=['Class'])
X = df.drop('Class',axis=1)
y = df['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Escalar los datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Puedes ajustar los hiperparámetros según tus necesidades
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)
mlp.fit(X_train, y_train)
# Hacer predicciones
y_pred = mlp.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()

#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión
tn, fp, fn, tp = conf_matrix.ravel()

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))

# DataFrame Vino
df = pd.read_csv('/content/winequality-white.csv')
df = df.dropna(subset=['quality'])
X = df.drop('quality',axis=1)
y = df['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Escalar los datos
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Puedes ajustar los hiperparámetros según tus necesidades
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)
mlp.fit(X_train, y_train)
# Hacer predicciones
y_pred = mlp.predict(X_test)
pred_series = pd.Series(y_pred)
# Graficar el histograma
plt.hist(pred_series, bins=10)  # Puedes ajustar el número de bins según tu preferencia
plt.xlabel('Clases')
plt.ylabel('Frecuencia')
plt.title('Histograma de Predicciones')
plt.show()
#-------------------CALCULO DE METRICAS--------------------
conf_matrix =confusion_matrix(y_test,y_pred)

# Extraer los valores de la matriz de confusión de manera más general
tn, fp, fn, tp = conf_matrix.ravel()[:4]

# Calcular Sensitivity (Recall)
sensitivity = tp / (tp + fn)

# Calcular Specificity
specificity = tn / (tn + fp)

# Calcular precisión (precision)
precision = tp / (tp + fp)

# Calcular recall (sensibilidad o recall)
recall = tp / (tp + fn)

# Calcular F1 Score
f1_score = 2 * (precision * recall) / (precision + recall)

# Mostrar los resultados
print('Precision: {:.2f}'.format(precision))
print('Accuracy: {:.2f}'.format(accuracy_score(y_test,y_pred)))
print('Sensitivity (Recall): {:.2f}'.format(sensitivity))
print('Specificity: {:.2f}'.format(specificity))
print('F1 Score: {:.2f}'.format(f1_score))