# -*- coding: utf-8 -*-
"""Act4_MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-A8NXhIDdMB6tWJQ9gQmIMFmLK63GdPa
"""

import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import random

# Función de activación sigmoide
def sigmoid(x):
    clipped_x = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-clipped_x))

def sigmoid_derivada(x):
    clipped_x = np.clip(x, -500, 500)
    return clipped_x * (1 - clipped_x)
def relu(x):
    return  max(0, x)

#Reemplazar valores
def sustituir(x):
  if x<0:
    return 0
  else:
    return 1

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def normalize_data(data):
    # Normalización min-max
    min_val = np.min(data, axis=0)
    max_val = np.max(data, axis=0)
    normalized_data = (data - min_val) / (max_val - min_val)
    return normalized_data, min_val, max_val

def MLP_entrenamiento(entrada_train, salida_train, neuronas_capas_ocultas, aprendizaje, epocas):
    num_entradas=4
    num_salidas=3
    salida_train = salida_train.tolist()
    num_muestras = len(entrada_train)
    num_capas_ocultas = len(neuronas_capas_ocultas)

    # Inicializar pesos y sesgos
    w_ocultas = [np.random.rand(num_entradas, neuronas_capas_ocultas[0])]
    b_ocultas = [np.random.rand(neuronas_capas_ocultas[0])]
    w_salida = [np.random.rand(neuronas_capas_ocultas[-1], num_salidas)]
    b_salida = [np.random.rand(num_salidas)]

    for i in range(1, num_capas_ocultas):
        w_ocultas.append(np.random.rand(neuronas_capas_ocultas[i-1], neuronas_capas_ocultas[i]))
        b_ocultas.append(np.random.rand(neuronas_capas_ocultas[i]))


    for _ in range(epocas):
      for l in range(num_muestras):
          # Propagación hacia adelante
          salida_oculta = sigmoid(np.dot(entrada_train[l], w_ocultas[0]) + b_ocultas[0])
          for i in range(1, num_capas_ocultas):
              salida_oculta = sigmoid(np.dot(salida_oculta, w_ocultas[i]) + b_ocultas[i])
          salida = sigmoid(np.dot(salida_oculta, w_salida[-1]) + b_salida[-1])

          # Calcular el error
          error = salida_train[l] - salida

          # Retropropagación para la capa de salida
          delta_salida = error * sigmoid_derivada(salida)
          w_salida[-1] += aprendizaje * np.outer(salida_oculta, delta_salida)
          b_salida[-1] += aprendizaje * delta_salida

          # Retropropagación para las capas ocultas
          delta_oculta = np.dot(delta_salida, w_salida[-1].T) * sigmoid_derivada(salida_oculta)
          for i in range(num_capas_ocultas-1, -1, -1):
              if i == 0:
                  entrada = entrada_train[l]
              else:
                  entrada = salida_oculta
              w_ocultas[i] += aprendizaje * np.outer(entrada, delta_oculta)
              b_ocultas[i] += aprendizaje * delta_oculta



    return w_ocultas, b_ocultas, w_salida, b_salida

datos_80_porcentaje = []
datos_20_porcentaje = []
# Abre el archivo en modo lectura
with open('/content/irisbin.csv', 'r') as file:
    cont_e=0
    cont_g=0
    for linea in file:
        # Almacena los elementos en los arreglos correspondientes
        if(cont_e<8):
          datos_80_porcentaje.append(linea)
          cont_e += 1
        else:
          datos_20_porcentaje.append(linea)
          cont_g += 1
          if(cont_g==2):
            cont_e=0
            cont_g=0
# Dividir cada línea en una lista de elementos
lista_valores = [linea.strip().split(',') for linea in datos_80_porcentaje]
# Crear un DataFrame a partir de la lista de listas
datos_80_porcentaje = pd.DataFrame(lista_valores,columns=['A','B','C','D','S1','S2','S3'])
datos_80_porcentaje[['A','B','C','D','S1','S2','S3']] = datos_80_porcentaje[['A','B','C','D','S1','S2','S3']].astype(float)
#datos_80_porcentaje =datos_80_porcentaje.applymap(sustituir)
datos_80_porcentaje.reset_index(drop=True, inplace=True)
entrada_train = datos_80_porcentaje[['A', 'B', 'C', 'D']].to_numpy()
salida_train = datos_80_porcentaje[['S1', 'S2', 'S3']].to_numpy()
#ENTRENAMIENTO
entrada_train_normalized, min_entrada, max_entrada = normalize_data(entrada_train)
salida_train_normalized, min_salida, max_salida = normalize_data(salida_train)
# Define el número de neuronas en las capas ocultas, tasa de aprendizaje y número de épocas
neuronas_capas_ocultas = [3, 3]
aprendizaje = 0.25
epocas = 150

# Llama a la función con los argumentos correspondientes
w_ocultas, b_ocultas, w_salida, b_salida = MLP_entrenamiento(entrada_train, salida_train, neuronas_capas_ocultas, aprendizaje, epocas)

datos_20_porcentaje = []
# Abre el archivo en modo lectura
with open('/content/irisbin.csv', 'r') as file:
    cont_e=0
    cont_g=0
    for linea in file:
        # Almacena los elementos en los arreglos correspondientes
        if(cont_e<8):
          #datos_80_porcentaje.append(linea)
          cont_e += 1
        else:
          datos_20_porcentaje.append(linea)
          cont_g += 1
          if(cont_g==2):
            cont_e=0
            cont_g=0
# Dividir cada línea en una lista de elementos
lista_valores = [linea.strip().split(',') for linea in datos_20_porcentaje]
# Crear un DataFrame a partir de la lista de listas
datos_20_porcentaje = pd.DataFrame(lista_valores,columns=['A','B','C','D','S1','S2','S3'])
datos_20_porcentaje[['A','B','C','D']] = datos_20_porcentaje[['A','B','C','D']].astype(float)
#datos_20_porcentaje =datos_20_porcentaje.applymap(sustituir)
datos_20_porcentaje.reset_index(drop=True, inplace=True)
entrada_test = datos_20_porcentaje[['A', 'B', 'C', 'D']].to_numpy()
# Aplica la normalización a tus datos de entrada y salida
entrada_normalized, min_entrada, max_entrada = normalize_data(entrada_test)
# Si deseas hacer predicciones después del entrenamiento, recuerda normalizar tus entradas antes de pasarlas por la red
def predict(input_data):
    # Normaliza las entradas utilizando los mismos valores de min y max que usaste durante el entrenamiento
    input_data_normalized = (input_data - min_entrada) / (max_entrada - min_entrada)

    # Propagación hacia adelante para obtener las predicciones
    salida_oculta = sigmoid(np.dot(input_data_normalized, w_ocultas[0]) + b_ocultas[0])
    for i in range(1, len(neuronas_capas_ocultas)):
        salida_oculta = sigmoid(np.dot(salida_oculta, w_ocultas[i]) + b_ocultas[i])
    salida = np.dot(salida_oculta, w_salida[-1]) + b_salida[-1]

    # Aplica softmax
    exp_salida = np.exp(salida - np.max(salida, axis=1, keepdims=True))
    softmax_salida = exp_salida / np.sum(exp_salida, axis=1, keepdims=True)

    # Desnormaliza las salidas antes de devolverlas
    output_data = softmax_salida * (max_salida - min_salida) + min_salida

    return output_data
salidas = predict(entrada_test)
for l in range(10):
  print(entrada_test[l], salidas[l])